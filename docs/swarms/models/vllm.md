# `vLLM` Documentation

## Table of Contents
- [Overview](#overview)
- [Installation](#installation)
- [vLLM Class](#vllm-class)
  - [Initialization](#initialization)
  - [Methods](#methods)
    - [run](#run)
- [Usage Examples](#usage-examples)
- [Common Issues and Troubleshooting](#common-issues-and-troubleshooting)
- [References and Resources](#references-and-resources)

---

### Overview <a name="overview"></a>

Welcome to the documentation for the vLLM (Variable-Length Language Model) library. vLLM is a powerful tool for generating text using pre-trained language models. This documentation will provide a comprehensive guide on how to use vLLM effectively.

#### Purpose

vLLM is designed to simplify the process of generating text using language models, specifically the Facebook `opt-13b` model. It allows you to fine-tune various parameters to achieve the desired text generation outcomes.

#### Key Features

- Seamless integration with the Facebook `opt-13b` language model.
- Flexible configuration options for model parameters.
- Support for generating text for various natural language processing tasks.

### Installation <a name="installation"></a>

Before using vLLM, you need to install swarms. You can install vLLM using `pip`:

```bash
pip install swarms vllm
```

### vLLM Class <a name="vllm-class"></a>

The vLLM class is the core component of the vLLM library. It provides a high-level interface for generating text using the Facebook `opt-13b` language model.

#### Initialization <a name="initialization"></a>

To initialize the vLLM class, you can use the following parameters:

- `model_name` (str, optional): The name of the language model to use. Defaults to "facebook/opt-13b".
- `tensor_parallel_size` (int, optional): The size of the tensor parallelism. Defaults to 4.
- `trust_remote_code` (bool, optional): Whether to trust remote code. Defaults to False.
- `revision` (str, optional): The revision of the language model. Defaults to None.
- `temperature` (float, optional): The temperature parameter for text generation. Defaults to 0.5.
- `top_p` (float, optional): The top-p parameter for text generation. Defaults to 0.95.

```python
from swarms.models import vLLM

# Initialize vLLM with default parameters
vllm = vLLM()

# Initialize vLLM with custom parameters
custom_vllm = vLLM(
    model_name="custom/model",
    tensor_parallel_size=8,
    trust_remote_code=True,
    revision="abc123",
    temperature=0.7,
    top_p=0.8,
)
```

#### Methods <a name="methods"></a>

##### run <a name="run"></a>

The `run` method is used to generate text using the vLLM model. It takes a `task` parameter, which is a text prompt or description of the task you want the model to perform. It returns the generated text as a string.

```python
# Generate text using vLLM
result = vllm.run("Generate a creative story about a dragon.")
print(result)
```

### Usage Examples <a name="usage-examples"></a>

Here are three usage examples demonstrating different ways to use vLLM:

**Example 1: Basic Text Generation**

```python
from swarms.models import vLLM

# Initialize vLLM
vllm = vLLM()

# Generate text for a given task
generated_text = vllm.run("Generate a summary of a scientific paper.")
print(generated_text)
```

**Example 2: Custom Model and Parameters**

```python
from swarms.models import vLLM

# Initialize vLLM with custom model and parameters
custom_vllm = vLLM(
    model_name="custom/model",
    tensor_parallel_size=8,
    trust_remote_code=True,
    revision="abc123",
    temperature=0.7,
    top_p=0.8,
)

# Generate text with custom configuration
generated_text = custom_vllm.run("Create a poem about nature.")
print(generated_text)
```

**Example 3: Batch Processing**

```python
from swarms.models import vLLM

# Initialize vLLM
vllm = vLLM()

# Generate multiple texts in batch
tasks = [
    "Translate the following sentence to French: 'Hello, world!'",
    "Write a short story set in a futuristic world.",
    "Summarize the main points of a news article about climate change.",
]

for task in tasks:
    generated_text = vllm.run(task)
    print(generated_text)
```

### Common Issues and Troubleshooting <a name="common-issues-and-troubleshooting"></a>

- **ImportError**: If you encounter an `ImportError` related to vLLM, make sure you have installed it using `pip install vllm`.

- **Model Configuration**: Ensure that you provide valid model names and configurations when initializing vLLM. Invalid model names or parameters can lead to errors.

- **Text Generation**: Be cautious with text generation parameters like `temperature` and `top_p`. Experiment with different values to achieve the desired text quality.

### References and Resources <a name="references-and-resources"></a>

For more information and resources related to vLLM and language models, refer to the following:

- [vLLM GitHub Repository](https://github.com/vllm/vllm)
- [Hugging Face Transformers Documentation](https://huggingface.co/transformers/)
- [Facebook `opt-13b` Model Documentation](https://huggingface.co/facebook/opt-13b)

---

This concludes the documentation for the vLLM library. We hope this guide helps you effectively use vLLM for text generation tasks. If you have any questions or encounter issues, please refer to the troubleshooting section or seek assistance from the vLLM community. Happy text generation!